import sys
import os
import argparse

# Add project root to sys.path
projectroot = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if projectroot not in sys.path:
    sys.path.insert(1, projectroot)

# components
from constants.pipeline import CheckpointKeys as CP_KEYS
from runner.utilities.io import IO
from runner.workflow.logger import MetricCSVLogger, MetricConsoleLogger
from runner.workflow.logger import EpochProgressLogger, TimeConsoleLogger
from runner.workflow.logger import CometMLLogger
from runner.workflow.engine import EngineUtility
from runner.workflow.checkpointer import Checkpoint

# assembly
import torch
from ignite.engine import Events
from typing import Tuple

# data
from torch.utils.data import DataLoader


def add_argparser():
    parser = argparse.ArgumentParser()
    parser.add_argument('-e','--experiment', type=str, required=True, help='Path to directory containing experiment description')
    parser.add_argument('-c','--clean', action="store_true", required=False, default=False, help='Refresh output folders generated by experiment')
    parser.add_argument('-r','--resume', action="store_true", required=False, default=False, help='Resume a paused experiment')
    parser.add_argument('--disable-comet', action="store_true", required=False, default=False, help='Disables comet.ml logging')
    parser.add_argument('-g','--gpu', required=False, nargs='+', default=[], help='Specify set of gpus')
    return parser


# core logic
def main():

    # parse and setup args
    parser = add_argparser()
    args, _ = parser.parse_known_args()
    if not os.path.exists(args.experiment):
        raise Exception("Experiment directory does not exist")
    if not os.path.isdir(args.experiment):
        raise Exception("Experiment path should point to a directory")

    # link to experiment folder
    sys.path.insert(0, args.experiment)
    from experiment import Hyperparameters as HP
    from experiment import ExperimentDescription
    from experiment import DatasetDescription

    splits_keys = ['experiment']
    name = splits_keys[0]

    args = parser.parse_args()

    # args saftey check
    if args.clean and args.resume:
        raise Exception("Cannot clean and resume")

    # setup device
    if len(args.gpu) == 0: devices = None
    else:
        devices = [torch.device(f'cuda:{gpu}') for gpu in args.gpu]
        torch.cuda.set_device(devices[0])

    if args.clean: IO.clean_folders(splits_keys, args.experiment)
    folders = IO.setup_folders(splits_keys, args.experiment)

    # Check if folders already contain something
    if not args.clean and not args.resume and IO.is_dirty_folders(splits_keys, args.experiment):
        raise Exception("Split folders contain data. Either use --clean or --resume to restart or continue an experiment.")

    metric_folder, checkpoint_folder = folders[name]

    # set up experiment descriptors
    experiment_descriptor = ExperimentDescription()

    # setup experiment objects
    experiment_descriptor.setup_devices(devices)
    experiment_descriptor.setup_objects()
    experiment_descriptor.setup_train_handlers()
    experiment_descriptor.setup_evaluate_handlers()
    experiment_descriptor.setup_train_metrics()
    experiment_descriptor.setup_evaluate_metrics()

    # core components
    models = experiment_descriptor.get_models()
    optimizers = experiment_descriptor.get_optimizers()
    schedulers = experiment_descriptor.get_schedulers()

    # process metrics
    packed_train_metrics = experiment_descriptor.get_train_metrics()
    packed_evaluate_metrics = experiment_descriptor.get_evaluate_metrics()

    # metric safety
    if packed_train_metrics is not None:
        train_metrics, train_metric_names = packed_train_metrics
    else: raise ValueError('At least one metric must be defined for training')

    if packed_evaluate_metrics is not None:
        evaluate_metrics, evaluate_metric_names = packed_evaluate_metrics
    else: raise ValueError('At least one metric must be defined for evaluation')

    # process metrics
    packed_train_handlers = experiment_descriptor.get_train_handlers()
    packed_evaluate_handlers = experiment_descriptor.get_evaluate_handlers()

    # handler safety
    if packed_train_handlers is not None:
        assert isinstance(packed_train_handlers, Tuple), "Train Handlers must be type Tuple or None"
        train_handlers, train_handler_states = packed_train_handlers
    else: train_handlers, train_handler_states = [], []

    if packed_evaluate_handlers is not None:
        assert isinstance(packed_evaluate_handlers, Tuple), "Evaluate Handlers must be type Tuple or None"
        evaluate_handlers, evaluate_handler_states = packed_evaluate_handlers
    else: evaluate_handlers, evaluate_handler_states = [], []


    dataset_descriptor = DatasetDescription()
    # TODO: setup this function in template.py
    dataset_descriptor.setup_datasets()
    train_set = dataset_descriptor.get_train_dataset()
    evaluate_set = dataset_descriptor.get_val_dataset()
    train_loader = DataLoader(train_set, batch_size=HP.batch_size, num_workers=HP.num_workers, pin_memory=True, shuffle=True)
    evaluate_loader = DataLoader(evaluate_set, batch_size=HP.batch_size, num_workers=HP.num_workers)

    # create default loggers and timers
    train_progress_logger = EpochProgressLogger(progress_freq=HP.progress_freq, header=f"{name} - Train Progress")
    train_csv_logger = MetricCSVLogger(os.path.join(metric_folder, 'train_metrics.csv'), HP.cp_period, len(train_loader))
    train_console_logger = MetricConsoleLogger(header=f"{name} - Train Metrics")
    train_time_logger = TimeConsoleLogger(header=f"{name} - Train Times")

    evaluate_progress_logger = EpochProgressLogger(progress_freq=HP.progress_freq, header=f"{name} - Validate Progress")
    evaluate_csv_logger = MetricCSVLogger(os.path.join(metric_folder, 'validate_metrics.csv'), HP.cp_period, len(train_loader))
    evaluate_console_logger = MetricConsoleLogger(header=f"{name} - Validate Metrics")
    evaluate_time_logger = TimeConsoleLogger(header=f"{name} - Validate Times")

    # assemble train engine and bind user metrics and handlers
    trainer = EngineUtility.assemble(experiment_descriptor.train_step, train_metrics, train_metric_names, train_handlers, train_handler_states)

    # prepare default train loggers
    train_progress_logger.attach(trainer)
    train_csv_logger.attach(trainer)
    train_console_logger.attach(trainer)
    train_time_logger.attach(trainer)

    # Assemble evaluation engine and bind user metrics and handlers
    evaluator = EngineUtility.assemble(experiment_descriptor.evaluate_step, evaluate_metrics, evaluate_metric_names, evaluate_handlers, evaluate_handler_states)

    # bind engine syncronization
    @evaluator.on(Events.EPOCH_STARTED)
    def sync_engine_state(evaluator):
        EngineUtility.sync_engine(trainer, evaluator)

    # bind evaluation to trainer
    @trainer.on(Events.EPOCH_COMPLETED)
    def evaluate(trainer):
        evaluator.run(evaluate_loader)

    # prepare default evaluate handlers
    evaluate_progress_logger.attach(evaluator)
    evaluate_csv_logger.attach(evaluator)
    evaluate_console_logger.attach(evaluator)
    evaluate_time_logger.attach(evaluator)

    # checkpoint description and init
    checkpoint_objects = {CP_KEYS.ENGINE : trainer,
                          CP_KEYS.MODELS : models,
                          CP_KEYS.OPTIMIZERS : optimizers,
                          CP_KEYS.SCHEDULERS : schedulers}

    cp_metric, cp_fn = HP.cp_stat
    if hasattr(HP, 'cp_save_period'):
        cp_save_period = HP.cp_save_period
    else:
        cp_save_period = 0
    checkpoint = Checkpoint(checkpoint_folder, name, checkpoint_objects, HP.cp_patience, HP.cp_period,
                            cp_metric, cp_fn, cp_save_period)
    assert cp_metric in evaluate_metric_names, "Checkpoint requires a validation metric"
    evaluator.add_event_handler(Events.EPOCH_COMPLETED, checkpoint)

    # resume option
    is_resuming = (args.resume and IO.is_dirty_folders([name], args.experiment))
    if is_resuming:
        state_checkpoint_file = os.path.join(checkpoint_folder, f'{name}_state_checkpoint.pth.tar')
        model_checkpoint_file = os.path.join(checkpoint_folder, f'{name}_model_checkpoint.pth.tar')
        checkpoint.resume(state_checkpoint_file, model_checkpoint_file, checkpoint_objects)

    # Comet ML Logger
    if not args.disable_comet:
        if args.resume_new:
            is_resuming = False
        HP_fields = {k:v for k,v in vars(HP).items() if '__' not in k }
        exp_name = f"{os.path.basename(os.path.abspath(args.experiment))}_{name}"
        cometml_logger = CometMLLogger(project_name=HP.project_name, exp_name=exp_name,
                exp_dir=args.experiment,
                exp_config=HP_fields, checkpoint_dir=checkpoint_folder,
                is_resuming=is_resuming, epoch_length=len(train_loader),
                workspace=HP.workspace)

        cometml_logger.attach(trainer)
        cometml_logger.attach(evaluator)

    # train and evaluate experiment
    if is_resuming:
        print("Resuming experiment...")
    else:
        print("Starting experiment...")
    torch.backends.cudnn.benchmark = True
    trainer.run(train_loader, max_epochs=HP.epochs)


if __name__ == '__main__':
    main()